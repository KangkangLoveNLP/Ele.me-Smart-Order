{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d024537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf1c204a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#模型选择\n",
    "tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"hfl/chinese-macbert-base\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0de2da",
   "metadata": {},
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9c90757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 路径\n",
    "\n",
    "data_path = r'D:\\code\\competiton\\饿了么智慧点餐\\意图识别\\train_with_zao.txt'#给出的训练集路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65bf6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, 'r') as f:\n",
    "    text,labels = [],[]\n",
    "    for line in f: \n",
    "        line = line.strip().split('\\t')\n",
    "        text.append(line[0].replace('[','').replace(']','').replace('\\'',''))\n",
    "        labels.append(line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "506d20bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11287 11287\n"
     ]
    }
   ],
   "source": [
    "labels,text = labels[1:],text[1:]\n",
    "labels = [int(i) for i in labels]\n",
    "print(len(labels),len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "598a33b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['来碗杨胃的走吧。',\n",
       " '天猫精灵要我皮蛋碎肉捏粥。',\n",
       " '天猫精灵.',\n",
       " '天猫精灵来碗红豆薏米养胃粥吧。',\n",
       " '天猫精灵药碗山药青菜养生粥吧。',\n",
       " '来碗热乎南瓜杨胃粥。',\n",
       " '我想听安尔晨的人生自古谁无情。',\n",
       " '14乘以0.4等于多少？',\n",
       " '天猫精灵播播放一首邝美云的堆积情感。',\n",
       " '天猫精灵你唱个小跳蛙吧。',\n",
       " '天猫精灵喜欢人是什么感觉？',\n",
       " '天猫精灵木耳炒肉片来一份。',\n",
       " '天猫精灵1.9722乘以7.2。',\n",
       " '天猫精灵162除以5等于多少？',\n",
       " '天猫精灵我你刚刚是被P麦了吗？',\n",
       " '天猫精灵，我想听周深的一燃易。',\n",
       " '我想看大然坊絮。',\n",
       " '天我再边声音到85，声音到85。',\n",
       " '天猫精点。😊',\n",
       " '天猫精灵来碗番茄面疙瘩汤吧。',\n",
       " '天猫精灵歌曲别看我只是一只羊。',\n",
       " '来碗热乎的皮带。',\n",
       " '天猫精灵来碗茶树菇老鸭汤面。',\n",
       " '天猫精灵要个香辣手撕包菜吧。',\n",
       " '天猫精灵来份豆角肉丝小炒吧。',\n",
       " '什么等于617加45等于几？',\n",
       " '天猫精灵2160乘以8。',\n",
       " '天猫精灵来份热乎里砂锅丸子，炖小白菜吧。',\n",
       " '天猫精灵来份酸锣卜炖老汤。',\n",
       " '刘，我在107加10等于多少？',\n",
       " '天猫精灵16.8乘25.2等于多少？',\n",
       " '我要上大他的城市飞车。',\n",
       " '点个红彩苔小草。',\n",
       " '天猫精灵1万38万888888881加1万8000亿等于多少？',\n",
       " '天猫精灵播放不遍放屁的声音。',\n",
       " '🎼天猫精灵啊，我在你说14减7加5等于多少？',\n",
       " '黄瓜炒香菇可以带午餐盒吗？',\n",
       " '天猫精灵我想放周深的小管我。😊',\n",
       " '天猫精灵来碗鸡肉玉米养生粥吧。',\n",
       " '天猫精灵请我播放公司的声音。',\n",
       " '天猫精灵要碗热乎的皮蛋瘦肉粥吧。',\n",
       " '天猫精灵要碗红枣红豆不蟹粥吧。',\n",
       " '天猫精灵15减6除以3等于几？',\n",
       " '🎼我想听胡彦斌的望客匆风。',\n",
       " '天猫精灵要个韭房肉丝小炒吧。',\n",
       " '天猫精灵来粉家常小草发财吧。',\n",
       " '天猫精灵人是怎么我宝粑的？😊',\n",
       " '天猫精灵12乘以6.1等于多少？',\n",
       " '天猫精灵关闭米小圈的那个报时。',\n",
       " '天猫精灵是阿里巴巴人工智能实验室于2017年7月5日发布的AI智能产品品牌。但。',\n",
       " '天猫精灵来碗红枣银耳热胡粥吧。',\n",
       " '天猫精灵要碗山药几丝养胃粥吧。',\n",
       " '天猫精灵遥望五国海密粥。',\n",
       " '精万雷奥特曼的声音。',\n",
       " '天猫精灵请播送六字大明咒。',\n",
       " '二人す。',\n",
       " '你的你功能太少了，我一点都不喜欢你了。',\n",
       " '天猫精灵刚才的故事继续走下去。',\n",
       " '好漂亮啊，天猫精灵哦，又服好漂亮都穿喽。',\n",
       " '天猫精灵合同履约成本和合同取得成本有什么区别？',\n",
       " '天猫精灵来份芹菜土豆丝吧。',\n",
       " '天猫精灵要碗暖性红豆粥扒。',\n",
       " '天猫精灵播放工程车啊，不对，播放挖掘机。',\n",
       " '来份家常大拌菜试试。',\n",
       " '天猫精灵来碗热乎的紫菜蛋化汤。',\n",
       " '天猫精灵播放厦门大学易中天。',\n",
       " '天猫精灵来玩红豆海米养生粥吧。',\n",
       " '天猫精灵来份青椒肉丝。',\n",
       " '猫精灵播放下那个。',\n",
       " '天猫精灵来粉部须肉炒菜大米饭吧。',\n",
       " '天猫精灵连接我的iphone。',\n",
       " '天猫精灵要个西红柿炒蛋吗？',\n",
       " '天猫精灵来碗热乎的西红柿鸡蛋汤面吧。',\n",
       " '天猫精灵请重新给我播放我刚刚听的东西。',\n",
       " '天猫精灵来份西红柿鸡蛋浇头饭。',\n",
       " '天猫精灵来份花生黑米养胃粥吧，养胃的。',\n",
       " '天猫精灵来碗鹌鹑蛋香菇肉丝汤吧。',\n",
       " '天猫精灵播放我的网易云音乐。',\n",
       " '天猫精灵0.914乘12等于多少？',\n",
       " '天猫精灵两2000乘2000等于多少？',\n",
       " '天猫精灵来份爆炒手撕包菜吧。',\n",
       " '天猫精灵，你认不认识小爱同学？',\n",
       " '天猫精灵，你还能不能播放一些声音？',\n",
       " '天猫精灵，你你咋没有你咋没有手了？',\n",
       " '天猫精灵，我想听张国威的生气。',\n",
       " '天猫精播放队的小跳蛙。',\n",
       " '天猫精灵咸鱼头炖豆腐汤来一份热乎的。',\n",
       " '天猫精灵青椒炒蛋来一份。',\n",
       " '放一首王心凌的我会滑宝的。',\n",
       " '天猫精灵74.9乘以2等于多少？',\n",
       " '天猫精灵要个芹菜木耳小炒。',\n",
       " '天猫精灵47减35等于几？',\n",
       " '来碗外婆秘制的八宝甜粥吧。',\n",
       " '天猫精灵点份黄金皮蛋瘦肉做吧。',\n",
       " '天猫精灵帮播放一首毛阿敏的渴望。',\n",
       " '天猫精灵来份西兰花香菇小炒。',\n",
       " '天猫精灵来碗香菇老鸭热烫。',\n",
       " '天猫精灵3600除以1600。',\n",
       " '天猫精灵来往3月入木走吧。',\n",
       " '天猫精灵来份清爽西脸花小草吧。',\n",
       " '天猫精灵来份胡萝卜肉丝小炒吧。',\n",
       " '天猫精灵700除以3等于多少？',\n",
       " '天猫精灵来碗热火的皮蛋瘦肉粥。',\n",
       " '天猫紧邻腰腕乱威的小蜜南瓜肘吧。',\n",
       " '天猫精灵进攻的反义词是什么？',\n",
       " '你叫林，江苏农牧科技职业技术学院。',\n",
       " '天猫精灵打开200200的会员。😊',\n",
       " '天猫精灵耀晚香遇你周末周。',\n",
       " '要碗红照三月洲养胃的。',\n",
       " '天猫精灵播放泰勒斯威。😔',\n",
       " '天猫精灵来碗热乎的皮蛋瘦肉粥吧。',\n",
       " '天猫精灵要份土豆豆角萌范吧。',\n",
       " '天猫精灵再音量小一点。',\n",
       " '天猫精灵来份蒜香小香菜吧。',\n",
       " '增大音量，天猫精灵增大音量。',\n",
       " '天猫精灵12.6加37.4等于多少？',\n",
       " '来碗竹笋老鸡汤。',\n",
       " '天猫精灵调到30%。',\n",
       " '天猫精灵有炖才吗？',\n",
       " '天猫精灵播放周深的缘起全前世今生版。',\n",
       " '天猫精灵来碗暖心八宝甜粥。',\n",
       " '那你最最最最最喜欢吃的是什么？',\n",
       " '天猫精灵来碗香菇肉末汤面吧。',\n",
       " '天猫精灵来份葱香娃娃菜吧。',\n",
       " '天猫精灵那盘相菇几市在做。',\n",
       " '要碗番茄鸡蛋热汤面。',\n",
       " '天猫精灵532.7加827.17等于多少？',\n",
       " '你等一会儿，天猫精灵关机，我还没换衣服的下回来。',\n",
       " '要份加沾西红柿蛋化汤。',\n",
       " '我等。',\n",
       " '天猫精灵要份小白菜炖嫩豆腐汤吧。',\n",
       " '嗯，天猫精灵我在你说听听男人亲女孩子的声音。😊',\n",
       " 'ワワ。',\n",
       " '来份萝卜玉米炖小排糖。',\n",
       " '天猫精灵给我播放中国好声音的歌曲。',\n",
       " '天猫精灵来碗热乎的香菇瘦肉粥。',\n",
       " '天猫精灵来份豆芽粉丝包吧，热乎的。',\n",
       " '天猫精灵来份蒜香油梅菜上烧。',\n",
       " '天猫精灵8。',\n",
       " '天猫精灵来碗家常排骨藕汤吧。',\n",
       " '天猫精灵，世界上最傻的人是谁？😊',\n",
       " '天猫精灵青豆玉米、肉末、小炒来一份吧。',\n",
       " '来碗热乎的黑玉汤，补补身子。',\n",
       " '侬好呀，要闻香喷喷的春花爆蛋，蛋炒得能调蟹哦。😔',\n",
       " '天猫精灵侬好，今朝想讨一份干煸豆角搭土豆喷香喷香饭，味道老灵光的，谢谢侬哦。',\n",
       " '天猫精灵le粉酸香嫩有麦菜。',\n",
       " '天猫精灵161的物理伴奏。',\n",
       " '天猫精灵14500除以0.95。',\n",
       " '天猫精灵给我关掉歌曲，要不然我就揍你。',\n",
       " '来份尖椒鸡蛋小炒。',\n",
       " '天猫精灵要温乎的饭菜。',\n",
       " '天猫精灵280除以8乘5等于。',\n",
       " '🎼我想象邓志舞的习惯很多。',\n",
       " '哑铃圣蹲钾的正确使用方法。',\n",
       " '天猫精灵提高音量到80%。',\n",
       " '要碗青菜肉末粥吧。',\n",
       " '天猫精灵播放你是陪我走过风雨的人。',\n",
       " '天猫精灵怎么绑定酷狗音乐账号？',\n",
       " '🎼我想听如今的火烧的寂寞。',\n",
       " '天猫精灵来碗风狂蛋花汤面吧。',\n",
       " '天猫精灵来碗面筋，白夜热汤。',\n",
       " '天猫精灵，我想听周深的灵牙之旅。',\n",
       " '天猫精灵来碗南瓜红薯甜粥吧。',\n",
       " '天猫精灵播放朱罗罗爆笑故事第310集。',\n",
       " '天猫精灵播放阿衰的咆哮日场。',\n",
       " '关开关开关开关开，中间的是开还是关。',\n",
       " '天猫精灵来份白菜灰木耳吧。',\n",
       " '把天猫精灵100个10加起来等于几？',\n",
       " '天猫精灵，我宝宝他又在头又在那里转来转去。',\n",
       " '天猫精灵，奶粉奶个香喷喷的，顺势这叫反吧。',\n",
       " '天猫精灵要碗热乎的皮蛋瘦肉粥吧。',\n",
       " '天猫精灵有美不油腻的。',\n",
       " '天猫精灵放60声音调到60。',\n",
       " '天猫精灵潜水不救是下点的水。',\n",
       " '来碗红豆黑米甜粥的。',\n",
       " '天猫精灵男人和女人吵架的声音。😡',\n",
       " '天猫精灵唱刘若英的当爱在靠近。',\n",
       " '要软烂一些的。',\n",
       " '天猫精灵要份荷包蛋鲜鱼汤。',\n",
       " '我想看超级宝贝汽车之歌。',\n",
       " '天猫精灵来份木耳藕片小草吧。',\n",
       " '覆盆子要加什么调料？',\n",
       " '天猫精灵要个原白菜炒粉条吧。',\n",
       " '要玩热乎的皮蛋收收手吧。',\n",
       " '天猫精灵要玩香甜南瓜粥。',\n",
       " '天猫精灵酷播音乐，我们一起来玩数。😊',\n",
       " '天猫精灵要个娃娃菜会嫩豆腐吧。',\n",
       " '天猫精灵把音量调到50，然后下一首。',\n",
       " '天猫精灵你5月17号是哪？',\n",
       " '天猫精灵播放张惠妹的当我开始偷偷想你。',\n",
       " '🎼天猫精灵93减78等于多少？',\n",
       " '只よ谁。',\n",
       " '天猫精灵番茄炒蛋奶粉。😔',\n",
       " '天猫精灵来份番茄炒蛋吧。',\n",
       " '天猫精灵600除以30等于多少？',\n",
       " '你好，天猫书房窗帘打开50%。',\n",
       " '天猫精灵来点带汤的饭。',\n",
       " '天猫精灵豆角茄子、小草来一份吧。',\n",
       " '天猫精灵462乘除以7等于多少？',\n",
       " '天猫精灵来碗饭茄牛肉汤面。',\n",
       " '天猫精灵把空调温度调低一点。',\n",
       " '天猫精灵来碗热乎的红枣黑米粥吧。',\n",
       " '天猫精灵来碗养胃的小米粥，隔着枸杞。',\n",
       " '天猫精灵要玩永威的小米主板。',\n",
       " '播放李文瀚给孩子买鞋。',\n",
       " '天猫精灵来份鸡丝炖娃菜吧。',\n",
       " '180加200等于多少？',\n",
       " '天猫精灵来份包菜炒粉丝吧。',\n",
       " '天猫精灵骷髅这个词组的是什么？',\n",
       " '天猫精灵来。',\n",
       " '猫精放波机声音大一点。',\n",
       " '15南瓜小米杨味粥。',\n",
       " '天猫精灵来碗热乎的皮蛋瘦肉粥。',\n",
       " '天猫精灵请把声音关小一点，声音关小一点。😊',\n",
       " '播放神奇鸡蛋人。',\n",
       " '2075除以45。',\n",
       " '杨艳林两个玉米加两个玉米等于几？😊',\n",
       " '天猫精灵来碗西红柿鸡蛋浇饭吧。',\n",
       " '猫精灵音量调音量调到40%。',\n",
       " '放一首年少的你啊。',\n",
       " '什么鬼？天猫精灵帮我设置一个一分钟后的起床闹钟。',\n",
       " '这皮卡丘就是你的孩子了。😊',\n",
       " '天猫精灵9万乘9万9999等于几？',\n",
       " '天猫精灵来碗山药肉帽暖胃粥，热乎的。',\n",
       " '天猫精灵要个7鸡蛋看吧。',\n",
       " '🎼你好，天猫声音开至30%。',\n",
       " '天猫精灵，我要听杨伟宗的我想要。',\n",
       " '天猫精灵来碗宣味三食汤吧。',\n",
       " '天猫精灵冬瓜排骨汤泡饭来一份。',\n",
       " '天猫精灵要那个爸妈常说的白米饭。',\n",
       " '天猫精灵来碗热乎的皮蛋瘦肉粥吧。',\n",
       " '天猫精灵要份儿萝卜炖排骨汤。',\n",
       " '天猫精灵声音放到70%。',\n",
       " '勿要一份老甜南瓜面汤，今朝想吃点甜蜜蜜子，侬帮我摆得适得侬点。',\n",
       " '天猫精灵你说17乘以7等于多少？',\n",
       " '咬个青菜柔愿粉丝糖吧。',\n",
       " '交给他天猫精灵天空为什么是蓝色的？',\n",
       " '天猫精灵来份菌骨丸子汤。',\n",
       " '天猫精灵来碗暖心紫菜蛋花汤吧。',\n",
       " '天猫精灵来碗红枣黑米养胃粥吧。',\n",
       " '天猫精灵来碗热虎皮蛋瘦肉粥吧。',\n",
       " '天猫精灵要个酸酸甜甜的大白菜吧。',\n",
       " '马经理，我今年刚满18岁了。',\n",
       " '天猫精灵来碗鲜香鲫鱼炖豆腐汤吧。',\n",
       " '天猫精灵来碗菌骨老牙汤，补补身子。',\n",
       " '来帮我来一份麻油香喷喷土豆搭小白菜，要热腾腾，谢谢侬。',\n",
       " '天猫精灵地方鱼塘是不是对的？',\n",
       " '天猫精灵要碗热乎的紫菜蛋花虾皮粥。',\n",
       " '天猫精灵播放迪迦奥特曼叫声。😊',\n",
       " '아 그 건강 하가.',\n",
       " '我想听都比上来的秋天。',\n",
       " '13.2乘以32除以44。',\n",
       " '天猫精灵来玩养胃的小米乐粥。',\n",
       " '好天猫精灵小朋小朋友笑的声音。😊',\n",
       " '天猫精灵要碗紫薯燕麦营养粥。',\n",
       " '🎼我要唱半分兄弟的爱如潮水。',\n",
       " '你见过神道大天的游戏吗？刚开始你是个破猴子，还有熊猫。',\n",
       " '我想听阮云飞的爱不爱我都可以。😊',\n",
       " '天猫精灵来碗热乎阳胃的鱼片粥吧。',\n",
       " '来碗暖威的小米南瓜粥吧。',\n",
       " '天猫精灵播放下一集0018。',\n",
       " '天猫精灵咸汤金针菇猪娃娃菜来一份。',\n",
       " '天猫精灵要奋城养生粥。',\n",
       " '天猫精灵来碗香菇瘦肉粥吧。',\n",
       " '天猫精灵13减7再加4等于。',\n",
       " '天猫精灵6加7再加8等于几？😊',\n",
       " '天猫精灵不要太油腻的。',\n",
       " '🎼天猫精灵播放歌曲，我有一个好爸爸。',\n",
       " '天猫精灵来份泰式鲜虾仁小炒吧。',\n",
       " '天猫精灵，我想听汪之泷的吉他歌。',\n",
       " '你不说你的同学你交换吗？',\n",
       " '天猫精灵要个玉米排骨养生汤吧。',\n",
       " '天猫精灵来份西红柿鸡蛋盖饭吧。',\n",
       " '天猫精灵来碗热乎的小米南瓜粥吧。',\n",
       " '天猫精灵要往咸县皮潭速舍走吧。',\n",
       " '天猫精灵要碗番茄蛋花汤。',\n",
       " '天猫精灵，我想听嗯快速汽车的声音。',\n",
       " '天猫精灵等我们把音花的声音。',\n",
       " '放小一点不放啦。',\n",
       " '天猫精灵要不暖心段胃的南瓜小米粥。',\n",
       " '我要唱陈瑞的，我是你不爱的人。',\n",
       " '天猫精灵要份圆中鸽子汤炖的暖乎的。',\n",
       " '来份蒜庙鸡汤小抄吧。',\n",
       " '天猫精您来往3月南瓜粥，热乎养胃的。',\n",
       " '要份青菜先蛋粥吧。',\n",
       " '天猫精灵说他妈1111集。',\n",
       " '我要唱空城的痛苦入了骨。',\n",
       " '天猫精灵来碗红豆薏米养胃州。',\n",
       " '来份粉条炒黄豆芽吧。',\n",
       " '天猫精灵来碗热乎的燕麦薏米养胃粥吧。',\n",
       " '🎼自动按摩你需要怎么练？',\n",
       " '天猫精灵煮的烂糊点的。',\n",
       " '天猫精灵来份排骨炖芋头汤吧。',\n",
       " '天猫精灵来碗杂粮养生粥吧。',\n",
       " '天猫精灵来玩南瓜小米甜粥吧。',\n",
       " '天猫精灵16乘34除以4等于多少？',\n",
       " '天猫精灵打开空调温度调至制冷模式。',\n",
       " '🎼我想通过跑步高手运动。',\n",
       " '调到最高天猫精灵让扫地机器人出来扫地。',\n",
       " '煮的南湖点的菜。',\n",
       " '天猫精灵螃蟹为什么会横走呢？',\n",
       " '天猫精灵唱爸爸的爸爸是谁呀？',\n",
       " '天猫精灵退一下吧，因为我挺挺累了。',\n",
       " '天猫精灵要个韭菜炒蛋吧。',\n",
       " '，你放一下那个欢乐好声音里头的插曲呗。',\n",
       " '思考鳕鱼柳与烤蔬菜对健身有帮助吗？',\n",
       " '看看嗯，32减6等于几？',\n",
       " '天猫精灵播放吴亦凡处刑结果。',\n",
       " '天猫精灵是谁让你三点半去唱歌的？',\n",
       " '天猫精灵要玩养胃南瓜粥吧。',\n",
       " '天猫精灵要份西红柿鸡蛋捞面。',\n",
       " '拉勿6下趟能回。',\n",
       " '要碗西红柿卤子面。',\n",
       " '奶粉皮蛋瘦肉热粥。',\n",
       " '结合批量烹饪的饮食调整。',\n",
       " '天猫精灵给小鸡蛋跳个舞吧。',\n",
       " '我想看小学满分作文。',\n",
       " '窗帘打开到30%。',\n",
       " '天猫精灵播放133集。',\n",
       " '天猫精灵来来暖味的小米粥吧。',\n",
       " '一天猫精灵16加3加1等于多少？😊',\n",
       " '天猫精灵打背包球的声音。😊',\n",
       " '会不会打怪兽？😊',\n",
       " '天猫精灵要个家长资产发卷吧。',\n",
       " '天猫精灵183减98。',\n",
       " '精灵120除以9等于几？',\n",
       " '来份蒜香粉丝蒸的是娃娃菜。',\n",
       " '天猫精灵呃，明天早上再播放吧，今天晚上我要睡觉去。😊',\n",
       " '天猫精灵来份家常木耳小炒吧。',\n",
       " '我想听于周然的鹅仿他。',\n",
       " '谢谢你，你可以给我下红心吗？😊',\n",
       " '天猫精灵天猫呃，3.14乘25等于多？',\n",
       " '🎼叮报叮声音声音开到50%。',\n",
       " '来份家常番茄炒蛋吧。',\n",
       " '天猫精灵播放安龙和朋友们。😊',\n",
       " '天猫精灵来份老北京石叔大杂烩吧。',\n",
       " '天猫精灵红枣冰糖甜蜜粥来一颗好。今朝嘴巴里有眼甜蜜蜜，吃眼甜津晶的么子，适宜适宜。',\n",
       " '精灵放光头强放熊大的声音。😊',\n",
       " '天猫精灵唱爱自己，方阔长命百岁。',\n",
       " '我想听许兮兮的新年新辉煌。',\n",
       " '天猫精灵什么奥特曼之贝利亚的儿子。',\n",
       " '天猫精灵来碗鸡骨汤炖粉丝吧。',\n",
       " '来碗热火的汤。😔',\n",
       " '推荐点好消化的。',\n",
       " '来碗山月排骨粥杨扬威。',\n",
       " '明天早上6点1。',\n",
       " '我想听李嘉威的非常。',\n",
       " '天猫精灵扫地机人开。',\n",
       " '找晃动版训练教学。',\n",
       " '中间天猫精灵播放斗罗大陆三龙王传说试听部分。',\n",
       " '天猫精灵六碗乐夫的老鸭粉丝汤吧。',\n",
       " '想吃点暖胃的。',\n",
       " '天猫精灵播放天周深天空音乐。',\n",
       " '天猫精灵仰天长啸出门去。下一句是？',\n",
       " '天猫精灵帅乡镇油麦菜来以份吧。',\n",
       " '，精灵55乘10等于多少？',\n",
       " '天猫精灵200乘150等于多少？',\n",
       " '要碗南瓜、小米粥养胃的。',\n",
       " '天猫精灵播放我喜欢听的歌。',\n",
       " '🎼我想听杨菊璐的后续。',\n",
       " '天猫精灵来点乐乎点的。',\n",
       " '天猫精灵来玩南瓜小米粥，杨杨味。',\n",
       " '天猫精灵水温度调到36。',\n",
       " '天猫精灵冲动的冲的步骤是哪个？',\n",
       " '63减27等于多少？',\n",
       " '天猫精灵，我有几个闹钟提醒。',\n",
       " '天猫精灵要去县咖里去打烫。',\n",
       " '天猫精灵要碗红薯、枸杞、甜粥热乎的。',\n",
       " 'いいとと。',\n",
       " '天猫精灵别太憨啊。',\n",
       " '来粉清炒娃娃菜暖敷些的。',\n",
       " '天猫精灵2乘2乘6.5等于多少？',\n",
       " '天猫精灵41乘39等于多少？',\n",
       " '来碗杨威的小米南瓜粥吧。',\n",
       " '天猫精灵。',\n",
       " '天猫精灵来碗小米粥，加点鸡块吧。',\n",
       " '嗯，我们梳点老虎的尾巴是什么颜色？。',\n",
       " '音量调天猫精灵调整为一倍数。',\n",
       " '来点热乎的。',\n",
       " '天猫精灵不会牙口的食物。',\n",
       " '🎼天猫精灵，有时候你救了也也会把你扔进垃圾场。😊',\n",
       " '天猫精灵声音台温开到70%。',\n",
       " '我唱我是贝夜小淘气。😊',\n",
       " '天猫精灵55.45减47.15。',\n",
       " '天猫精灵，你的名字好特殊呀，你能不能取名字叫王雨琪呀？',\n",
       " '来碗红枣黑米养胃粥吧。',\n",
       " '天猫精灵我看包天翼的英雄之路。',\n",
       " '天猫精灵来玩养味的小米绿豆粥吧。',\n",
       " '要碗热乎的皮蛋瘦肉粥吧。',\n",
       " '天猫精灵来碗香菇牛肉热乎粥。',\n",
       " '天猫精灵二的20次方是多少？',\n",
       " '天猫精灵来份青椒土豆片吧。',\n",
       " '天猫精灵邓丽君的我只在乎你。',\n",
       " '天猫精灵这个4加上3加上4是是多少字？😊',\n",
       " '夜望南上北打真有酒。',\n",
       " '哎，天猫精灵老婆咬我算钱炮吗？',\n",
       " '天猫精灵，你是不是不想给我返回呀？',\n",
       " '天猫精灵来碗主菜炖汤吧。',\n",
       " '来碗萝卜炖排骨汤。',\n",
       " '我想听沈奕腾的春。',\n",
       " '天猫精灵把你的声音调到最大。',\n",
       " '天猫精灵来碗山药鸡溶粥热乎养胃的。',\n",
       " '要份番茄鸡蛋盖饭。',\n",
       " '放一首爱到1440。',\n",
       " '天猫精灵来玩青车比打岳腐走吧。',\n",
       " '天猫精灵，我问你呃什么样的小喇叭？',\n",
       " '来碗西红柿鸡蛋浇头饭。',\n",
       " '天猫精灵32减17等于几？',\n",
       " '天猫精灵来份家常菠菜炒蛋吧。',\n",
       " '天猫精灵刚才是不是你放的歌啊？',\n",
       " '天猫精灵94减339等于几？',\n",
       " '来碗热乎养胃小米粥。',\n",
       " '天猫精灵播放花园的歌，有小小的花园和大大的花园。还有。😊',\n",
       " '来份酸酸甜甜的番茄蛋花糖。',\n",
       " '天猫精灵音量调到75，并且关闭。',\n",
       " '天猫精灵来一碗番茄蛋汤吧。',\n",
       " '天猫精灵红袖可头糟乃衣还有味碟。',\n",
       " '我想看国庆节快乐。',\n",
       " '天猫精灵声音小一点啊，尽量小一点啊。',\n",
       " '点样升灵要换热户个猪肚帘呢咁暖暖位。',\n",
       " '天猫精灵播放放放爱心萌盒的故事第11集。',\n",
       " '天猫精灵要碗皮蛋瘦肉热粥。',\n",
       " '天猫精灵要个芹菜、香干炒肉丝吧。',\n",
       " '天猫精灵来份韭菜鸡蛋碎吧。',\n",
       " '天猫精灵我跟你说话，你咋不吱声呢？',\n",
       " '绳索反向飞鸟的训练怎么做？',\n",
       " '天猫顶灵来碗番茄鸡蛋、热汤面，糖多些。',\n",
       " '天猫精灵药碗南瓜小米粥养养胃。',\n",
       " '天猫精灵继续播放笑话一箩筐。',\n",
       " '天猫精灵74.668除以3。',\n",
       " '天猫精灵来碗热乎的皮，等受着走吧。',\n",
       " '天猫精灵要玩养胃的小米粥。',\n",
       " '天猫精灵来份儿酸酸甜甜的番茄炒蛋。',\n",
       " '天猫精灵空调开到制冷16度16度。',\n",
       " '要个青椒肉丝小炒吧。',\n",
       " '天猫精灵来份韭菜鸡蛋，香香锅吧，要热乎的。',\n",
       " '天猫精灵热水器跟温度调到38。',\n",
       " '来份原白菜粉丝小炒吧。',\n",
       " '天猫精灵，那我因为是你好朋友。😊',\n",
       " '天猫精灵声音开到50%。',\n",
       " '天猫精灵要一份热乎生的猪肉粥。',\n",
       " '天猫精灵要个仙君软子汤暖暖味。',\n",
       " '精灵24加27等于几？😊',\n",
       " '天猫精灵关闭窗帘70%。',\n",
       " '🎼我要唱琉璃怪的长篇日记。',\n",
       " '要碗番茄蛋花汤热乎的。',\n",
       " '要煮的软烂点的。',\n",
       " '天猫精灵来碗红枣乌鸡汤吧。',\n",
       " '天寞中人一望男生的北大真有酒是谁啦。',\n",
       " '天猫精灵100减减100等于。😊',\n",
       " '天猫精，你说大声点，我听不见。',\n",
       " '来碗南瓜小米粥吧。',\n",
       " '天猫精灵来碗酸酸甜甜的番茄汤面吧。',\n",
       " '天猫精灵要碗皮蛋鸡丝热粥。',\n",
       " '天猫精灵要碗热乎的百合南瓜甜粥吧。',\n",
       " '天猫精灵有糖水的吗？',\n",
       " '来碗山药红枣养胃粥吧。',\n",
       " '好天猫精灵声音稍稍微小一点。',\n",
       " '天猫精灵来玩暖暖的小米粥养养味。',\n",
       " '天猫精灵酱香手撕包菜来一份吧。',\n",
       " '天猫精灵盐碗儿青菜香菇肉末粥。',\n",
       " '来玩热乎的，小米南瓜粥吧。',\n",
       " '天猫精灵185除以3等于多少？',\n",
       " '天猫精灵来一份暖心紫菜蛋花汤。',\n",
       " '识你好，天猫播放喜马拉雅听书一人一礼与狗去修仙。',\n",
       " '天猫精灵，你咋又不一直给我播放奥特曼的声音？',\n",
       " '天猫精灵要份南家小米粥养胃的。',\n",
       " '天猫精灵，为什么哎小宝这么熬险呢？',\n",
       " '天猫精灵一个白菜豆腐炖粉头把。',\n",
       " '天猫精灵播放周深的天猫记忆商店。',\n",
       " '要碗热乎呢皮蛋瘦肉粥，养养胃。',\n",
       " '推荐点加长的。',\n",
       " '我想听曹颖的不止遗憾。',\n",
       " '天猫精灵来玩南瓜小米养胃粥吧。',\n",
       " '天猫精灵来碗番茄炖牛肉面吧。',\n",
       " '我想听赵赵的路上歌。',\n",
       " '天猫精灵播放安河桥宋冬野。',\n",
       " '3、天猫精灵主人，你说45减7等于多少？',\n",
       " '天猫精灵来玩些蛋花汤吧。',\n",
       " '天猫精灵来碗香喷喷的稻花米饭吧。',\n",
       " '天猫精灵来一个小燕子的声音。',\n",
       " '天猫精灵30除以24等于多少？',\n",
       " '除以6，天猫精灵4.8除以5。',\n",
       " '天猫精灵来碗热的蘑菇鸡丝汤面。',\n",
       " '我先睡，让天猫精灵休息啊，休息吧，快点。',\n",
       " '天猫精灵来玩蓝心八宝杂粮粥吧。',\n",
       " '🎼我要唱张倩的，容意活着。',\n",
       " '天猫精灵，你可不可以给我们一点好处？',\n",
       " '🎼如何选择准类水分比的项目？',\n",
       " '天猫精灵来万勤缴抄单，该缴翻吧。',\n",
       " '天猫精灵要往红枣花生奶粥吧。',\n",
       " '天猫精灵要换青椒肉丝小炒啊。',\n",
       " '听猫毡快猜稍巢吧。',\n",
       " '天猫精灵要碗热乎的皮蛋瘦肉粥吧。',\n",
       " '天猫精灵西兰花小炒菜一份。',\n",
       " '天猫精灵11除以41等于多少？',\n",
       " '来碗南瓜小米粥。',\n",
       " '我我以前有个好朋友，今天生病了。',\n",
       " '后天的太阳是下雨，后天没有太阳。',\n",
       " '来碗皮蛋鸡茸粥。',\n",
       " '天猫精灵呃，你可以重启一下吗？',\n",
       " '天猫精灵来份青椒香干小草吧。',\n",
       " '天猫精灵王者荣耀中吕布的职能是什么？',\n",
       " '天猫精灵有煮的软的吗？',\n",
       " '🎼天猫精灵要玩热乎的皮带售后手。',\n",
       " '我想看欢。',\n",
       " '天猫精灵鲫鱼豆腐汤来一份热乎的。',\n",
       " '天猫精灵请放请播放男生唱的奇迹感线。',\n",
       " '天猫精灵连续播放狗叫的声音。',\n",
       " '，帮我来一份希得兰的猛烧大白菜，大白菜烧得苏店，勿要太硬，谢谢。😔',\n",
       " '天猫精灵来碗紫薯燕麦养胃粥吧。',\n",
       " '要个小粉桃喝菜吧。😔',\n",
       " '天猫精灵2加1加1加1等于几？',\n",
       " '天猫精灵放一篇宋朝的解说。',\n",
       " '嗯，还要天猫精灵啊呃孙悟空的声音。',\n",
       " '天猫精灵来份番茄鸡蛋盖饭。',\n",
       " '奶粉酸香油麦菜要清淡的。',\n",
       " '天猫精灵来份干锅炒花菜吧。',\n",
       " '天猫精灵来份香辣寿司包菜吧。',\n",
       " '天猫精灵要碗玉米青菜粥吧。',\n",
       " '天猫精灵请将声音调到50%。',\n",
       " '🎼蔬菜鸡肉走丝怎么搭配更合适？',\n",
       " '天猫精灵来份紫菜粉丝热汤。',\n",
       " '天猫精灵播放张杰的孤独者。',\n",
       " '来份清清爽爽的小油麦菜吧。',\n",
       " '天猫精灵分首今天天气真好。😊',\n",
       " '天猫精灵来份白菜豆腐包，热乎的。',\n",
       " '在线查找双臂推举和训练。',\n",
       " '先来个慢跑晚支撑锻炼。',\n",
       " '天猫精灵3.3乘以0.99。',\n",
       " '天猫精灵来份玉林家常拌菜吧。',\n",
       " '天猫精灵来碗暖暖的黑米羊贝粥吧。',\n",
       " '天猫精灵来份香辣手撕爆菜锅。',\n",
       " '天猫精灵，你是我的小呀小苹果。',\n",
       " '天猫精灵要玩南瓜小米养痿粥热乎的。',\n",
       " '你倒说呀，你听什么呀？',\n",
       " '天猫精灵这个女人叫胡新宇。',\n",
       " '天猫精灵连续播放鸭子的声音。',\n",
       " '天猫精灵勒马优图是什么公司？',\n",
       " '天猫精灵别放辣呀。',\n",
       " '要个芹菜拌小虾米吧。',\n",
       " '天猫精灵韭菜炒蛋来一份吧。',\n",
       " '天猫精灵来份韭菜煎蛋吧。',\n",
       " '天猫精灵来份酥肉豆腐包吧。',\n",
       " '🎼我要上张杰的天天想你。',\n",
       " '来盘蒜香嫩油麦菜尝尝。',\n",
       " '来份土豆烧牛肉盖饭吧。',\n",
       " '天猫精灵，难道你把你的图片成在坦布尔他家了吗？😊',\n",
       " '天猫精灵清淡写的菜。',\n",
       " '天猫精灵把餐厅灯光调到最亮。',\n",
       " '天猫精灵来碗香喷喷的皮蛋瘦肉大粪粥。',\n",
       " '天猫精灵要个鲜鱼豆腐汤吧。',\n",
       " '天猫精灵200减40减37减32等于多少？',\n",
       " '天猫精灵来碗红枣桂圆养心粥吧。',\n",
       " '天猫精灵来碗红枣黑米养胃粥吧。😊',\n",
       " '天猫精灵下一首下一首下一首下一首下一首。',\n",
       " '来碗热乎的皮蛋瘦肉粥吧。',\n",
       " '9980万减20减20减20减20减20加5加5加5加5。',\n",
       " '天猫精灵要碗香菇、皮蛋、香米粥吧。',\n",
       " '天猫精灵来份番茄炒蛋吧。',\n",
       " '天猫精灵来份番茄糖花汤。',\n",
       " '因为我觉得你没那么好看吧。',\n",
       " '天猫精灵取消当前的些儿童节目。',\n",
       " '天猫精灵来份番茄炒蛋饭吧。',\n",
       " '天猫精灵，我想给你带走黑腰给你捆住，杀死你。😊',\n",
       " '天猫精灵要个家常手撕包菜吧。',\n",
       " '天猫精灵来份韭菜炒蛋吧。',\n",
       " '播放精灵吃之我有一家4域屋。',\n",
       " '天猫精灵来碗热乎养胃的黑米粥吧。',\n",
       " '天猫精灵10万10万加百10万加百万等于几？',\n",
       " '晕天猫精灵晕头转向的晕怎么写？',\n",
       " '天猫精灵给我放阿琪拉的歌。😊',\n",
       " '天猫精灵要碗仙阁疙瘩汤。',\n",
       " '天猫精灵点个外婆菜炒蛋吧。',\n",
       " '天猫精灵藏尸体的地方在哪里？',\n",
       " '天猫精灵74.05减去63.75等于多少？',\n",
       " '天猫精灵，我想流血的声音流血声音。😊',\n",
       " '天猫精灵搜索漫画，我却爱着一个他。',\n",
       " '当前音量是多少？',\n",
       " '猫森雷播放我是一只猫，从来不起床。',\n",
       " '天猫精灵来碗养胃的小米粥，热乎的。',\n",
       " '天猫精灵来碗热乎的一片粥。',\n",
       " '来碗番茄鸡蛋汤面吧。',\n",
       " '天猫精灵8192加8192等于多几？',\n",
       " '天猫精灵来碗酸香番茄疙瘩汤吧。',\n",
       " '天猫精灵，为什么你的呼吸都在闪烁？😔',\n",
       " '猫，我想听泽塔奥特曼主题曲。😊',\n",
       " '天猫精灵100个奥艾克斯奥特曼的声音。😊',\n",
       " '天猫精灵要一份茄子肉末盖饭吧。',\n",
       " '天猫精灵来份番茄炖牛腩吧。',\n",
       " '初学者力量训练带训练讲解。',\n",
       " '天猫精灵来碗西红柿蛋花汤吧。',\n",
       " '拉要一份手司包菜小草，清爽点，勿要油，谢谢侬。',\n",
       " '天猫精灵把音量调到6%。',\n",
       " '天猫精灵来碗绿豆甜粥吧。',\n",
       " '天猫精灵来碗金瓜小米养胃粥吧。',\n",
       " '天猫精灵播放邓宇君等什么君的歌。',\n",
       " '激动毛巾的练习计划。',\n",
       " '天猫精灵扫地机器人看得清扫。',\n",
       " '天猫精灵来粉青椒鸡蛋盖饭吧。',\n",
       " '天猫精灵，你觉得重返未来中。',\n",
       " '天猫精灵音量响一点点可以吗？',\n",
       " '快速学会头侧推挤的方法。',\n",
       " '不对，听不见能这不对，我要果果收银。😡',\n",
       " '我想看我们的法则精彩集锦。',\n",
       " '天猫精灵来份四川凉拌菜吧。',\n",
       " '来粉山药摩尔小草。',\n",
       " '天猫精灵音量调至至80%。',\n",
       " '天猫精灵来份锡纸帮嫩麻娃菜清淡的。',\n",
       " '天猫精灵来碗菌菇汤吧，养养身子。',\n",
       " '天猫精灵，你们次都喜欢我了吗？😊',\n",
       " '天猫精灵来碗热乎的皮蛋瘦肉粥吧。',\n",
       " '天猫精灵来份蒜香小青菜儿吧。',\n",
       " '拉要一份砂果粉丝软瓜，热腾腾的，今朝想吃点暖胃子。',\n",
       " '天猫精灵6.28乘以1.5。',\n",
       " '天猫精灵韭菜鸡蛋小炒来一份吧。',\n",
       " '599乘8等于几？',\n",
       " '来碗红糖，咱俩甜粥吧。',\n",
       " '天精灵3呃33乘以12等于多少？',\n",
       " '天猫精灵木耳鸡蛋烩黄瓜片来一份吧。',\n",
       " '订个韭菜鸡蛋炒炒吧。',\n",
       " '天猫精灵放我们的祖国是花园。',\n",
       " '天猫精灵嘻谢了，都停止播放了。',\n",
       " '青蛙精灵嗯过了20分钟之后，请洗完玩发条绩跟我玩。😊',\n",
       " '你要乖乖啊，你要乖的啊。',\n",
       " '来碗鲜虾暖胃粥吧。',\n",
       " '好吧，那为什么你的声音那么像猫的？',\n",
       " '天猫精灵音量调到60%。',\n",
       " '天猫精灵，我有一个大砍刀姐姐给我的。😊',\n",
       " '来碗家常小羊疙瘩汤吧。',\n",
       " '天猫精灵要一份莴笋丝小炒。',\n",
       " '🎼播放宝宝巴士奇妙汽修师。',\n",
       " '天猫精灵放一首雅生的玫瑰。',\n",
       " '相不见言，马桶人马桶人的声音。😊',\n",
       " '我3点50分要和朋友去看个电影。',\n",
       " '天猫精灵来碗南国山药原胃州吧，热乎的。',\n",
       " '天慕真人来问自红肉们给钱话吧。',\n",
       " '天猫精灵，我考你言来互相尊重是哪个字？',\n",
       " '天猫精灵57加32等于几？😊',\n",
       " '天猫精灵酸萝卜炖老鸭汤来一份吧。',\n",
       " '天猫精灵imppo是什么意思？',\n",
       " '🎼我想听刘思健的圣尼古拉斯的雪天。',\n",
       " '🎼天猫精打开网易云，我喜欢的音乐。',\n",
       " '天猫精灵汤汤水水的就行。',\n",
       " '天猫精灵音量调到50%。',\n",
       " '天猫精灵把证据亮度加到80。',\n",
       " '天猫精灵来碗红枣枸杞、羊胃粥。',\n",
       " '天猫精灵来玩三峡香菇油门走吧。',\n",
       " '大一点声，天猫精灵大一点声音。',\n",
       " '天猫精灵80加40再加20。',\n",
       " '天猫精灵空调风速调低。',\n",
       " '天猫精灵播放化身孤岛的鲸。',\n",
       " '天猫精灵天猫那个摄像头从哪打开？',\n",
       " '豆角茄子来一份。',\n",
       " '天猫精灵18乘3.14等于多少？',\n",
       " '天猫精灵来份暖心八宝杂粮粥，热乎的。😊',\n",
       " '天猫精灵来份热乎的鸡蛋瘦肉粥。',\n",
       " '我在你生。',\n",
       " '天猫精灵锲而不舍的反义词。',\n",
       " '天猫精灵要一份蔬菜瘦肉皮蛋粥。',\n",
       " '天猫精灵唱一首毛不易的毛不易的毛不易的。',\n",
       " '天猫精灵炎热炎日的炎怎么写？😊',\n",
       " '天猫精灵请播放拉布拉多警察。',\n",
       " '做者夹胸花庭视频教材栏里样。',\n",
       " '来一伙的皮蛋瘦走吧。',\n",
       " '来红热乎的牛肉豆腐汤吧。',\n",
       " '天猫精灵来碗热敷的南瓜小米养胃粥。',\n",
       " '天猫精灵，我在找另外一个天猫精灵。😊',\n",
       " '天猫精灵把声音开到100%。',\n",
       " '天精灵重新播放云南打歌舞。',\n",
       " '天猫精灵来份饭茄面疙瘩汤。',\n",
       " '来点家菜。',\n",
       " '天猫精灵来份南瓜银耳热胡汤。',\n",
       " '几岁了，对不对？天猫精灵356乘439。',\n",
       " '什么你的答案是你答案是什么呀？😊',\n",
       " '天猫精灵要玩黄小米粥热虎的。',\n",
       " '天猫精灵来碗番茄蛋花汤吧。',\n",
       " '天猫精灵来碗鲜味蛤蜊疙瘩汤吧。',\n",
       " '天猫精灵播放刀剑的5点20。',\n",
       " '我要听周深的歌，雪花落下。',\n",
       " '天猫精灵，那份老北京秋里轮回烫吧。',\n",
       " '🎼我要唱光泽的背鹰。',\n",
       " '播放俺是河南人，河南洛阳人。',\n",
       " '我要唱大双二双的，你在他乡还好吗？',\n",
       " '你天猫精灵，你冷落我，我就放弃你。',\n",
       " '来份健康实蔬拌菜吧。',\n",
       " '天猫精灵来碗牛肉鹅蛋粥吧。',\n",
       " '天猫精灵1000除1等于多少？',\n",
       " '天猫精灵来碗热乎的皮蛋瘦肉粥。',\n",
       " '天猫精灵嗯播放一曲呃。',\n",
       " '天猫精灵来份莴笋丝小炒吧。',\n",
       " '天猫精灵来份番茄炒蛋。',\n",
       " '人的叫声是什么？是什么样的？',\n",
       " '天猫精灵连续播放猴子叫的声音。',\n",
       " '天猫精灵要给蘑菇炒小青菜吧。',\n",
       " '跳这灵，336除以3等于多少？',\n",
       " '奶粉独角肉是小炒吧。',\n",
       " '天猫精灵来碗白菜豆腐鱼丸热汤。',\n",
       " '天猫精灵，你在说话的话，我给你打一顿。😊',\n",
       " '天猫精灵，你叫你妈妈给你做。',\n",
       " '天猫精灵要个西兰花草肉片吧。',\n",
       " '天猫精灵来碗红枣红糖粥，有锅的就行。',\n",
       " '天猫精灵要一份招牌瘦乳皮蛋粥。',\n",
       " '请1907加8加9加9减8等于几？😊',\n",
       " '我说我不喜欢你，我喜欢小爱同学。',\n",
       " '天猫精灵来碗番茄面疙瘩汤。',\n",
       " '天猫精灵把这首歌增加到我喜欢里面。',\n",
       " '哎呀，天猫精灵真的是惹我生气。',\n",
       " '播放狗叫的声音。',\n",
       " '来碗香菇面筋汤面吧。',\n",
       " '天猫精灵奶粉变得香香的小青菜吧。',\n",
       " '🎼我想看先生你哪位？',\n",
       " '天猫精灵来份香菇炒小油菜吧。',\n",
       " '天猫精灵要个下仁山腰小炒吧。',\n",
       " '🎼我想看小升初总复习。',\n",
       " '天猫精灵来份清清爽爽的小青菜吧。',\n",
       " '就是我们是公安机看的，我们是假章。😊',\n",
       " '用坚固化身器怎么锻炼？',\n",
       " '天猫精灵摇碗青菜香菇热粥。',\n",
       " '来碗酸香西红柿炖骨汤。',\n",
       " '天猫精灵来一个红鸟的声音。😊',\n",
       " '要啊杭州桂林黑米粥养胃的。',\n",
       " '天猫精灵买一个丁酸氢化可迪松软膏。',\n",
       " '天猫精灵那个老虎的声老牛的声音。',\n",
       " '天猫精灵要份山药排骨粥养味的。',\n",
       " '天猫精灵遥控器是不是在俺家里？😊',\n",
       " '天猫精灵要个山羊木耳小草吧。',\n",
       " '天猫精灵把台灯调成影院模式。',\n",
       " '天猫精灵把阳台窗帘设置成天黑自动关动窗帘。',\n",
       " '播放周深的歌不限。',\n",
       " '来份手撕包菜吧。',\n",
       " '啊，天猫精灵啊15乘以19等于多少？',\n",
       " '天猫精灵来份西辣花小炒吧。',\n",
       " '天猫精灵来放白花彩虹炸蛋吧。',\n",
       " '天猫精灵来碗红枣小米养胃粥热乎的。',\n",
       " '天猫精灵80乘以5加80乘以2加78乘以3等于多少？',\n",
       " '天猫精灵来碗皮蛋瘦肉热粥吧。',\n",
       " '天猫精灵来玩加长限子时改放吧。',\n",
       " '来碗热乎的西红柿鸡蛋面，疙瘩汤。',\n",
       " '天猫精灵红枣小米粥来一碗养胃的。',\n",
       " '播一下周深的歌同心圆。',\n",
       " '天猫精灵来份香喷喷的稻花米饭。',\n",
       " '你说。',\n",
       " '来玩暖胃的小米粥。',\n",
       " '来碗冬瓜皮蛋肉片汤。',\n",
       " '天猫精灵换一首换一首不好听。',\n",
       " '音乐放到50%。',\n",
       " '我想听刘水妍的荧花水窗。',\n",
       " '天猫精灵来往香菇奇尸热族吧。',\n",
       " '天猫精灵越剧全剧祥林嫂精彩风袁雪风等演唱。',\n",
       " '天猫精灵来我先做杭州养胃的。',\n",
       " '天猫精灵，请问你打电话需打给谁，就要谁弄弄那个电话吗？😊',\n",
       " '天猫精灵51除以12等于多少？',\n",
       " '天猫精灵要个番茄鸡蛋盖饭。',\n",
       " '天猫精灵44.5乘以0.62。',\n",
       " '天猫精灵要碗香甜的黑米杂粮粥吧。',\n",
       " '天猫精灵休息，我走了，我上班去。😊',\n",
       " '83除以8约等于。',\n",
       " '因为0窗帘开至70%。',\n",
       " '天猫精灵受人欺负了怎么办呢？😔',\n",
       " '天猫精灵要碗牛肉窝蛋粥吧。',\n",
       " '天猫精灵123乘以6.5。',\n",
       " '来碗红糖小米红豆粥吧。',\n",
       " '3上5毛钱怎么能买100多呢？',\n",
       " '天猫精灵夜盲症用什么什么药治疗啊？',\n",
       " '天猫精灵来份肉乎砂锅皮蛋瘦肉粥。',\n",
       " '天猫精灵叶公好龙是寓言故事吗？',\n",
       " '几百天猫精灵，我在你说500加500等于几百？😊',\n",
       " '天猫精灵来份要么茄子盖饭。',\n",
       " '要碗乐乎的菠菜猪嘎粥。',\n",
       " '天猫精灵800减去340。',\n",
       " '天猫精灵请关机。',\n",
       " '天猫精灵150分乘以0.3。',\n",
       " '天猫精灵来碗暖胃的红糖小米粥吧。',\n",
       " '天猫精灵我听猴子警长不小鸡第一季小鸡墩蹲和。',\n",
       " '天猫精灵来份逗皮鸡蛋小炒吧。',\n",
       " '天猫精灵把音量调调到43。',\n",
       " '天猫精灵药丸南瓜银耳枸杞粥，养养胃。',\n",
       " '天猫精灵来碗鲜香紫菜蛋黄汤吧。',\n",
       " '天猫精灵来份热乎的茶树菇炖老鸡汤吧。',\n",
       " '天猫精灵你你就给我聊今天发生的什么。',\n",
       " '对，天猫精灵播放流行歌曲小点声。',\n",
       " '动毒的人跑步训练有危险吗？',\n",
       " '天猫精灵来份山药片炒木耳吧。',\n",
       " '天猫精灵，那份转为原酿走吧。',\n",
       " '天猫精灵来份山药茯苓养生汤吧。',\n",
       " '天猫精灵来碗香菇滑稽小碗吧。',\n",
       " '天猫这里有一份特子祥内皮单走。',\n",
       " '天猫精灵73.85乘以2等于多少？',\n",
       " '怎么进来啊，我唱唱歌又不唱啊。',\n",
       " '天猫精灵播放马方凯的你别再说话。',\n",
       " '天猫精灵来玩红枣小米南瓜泉粥吧。',\n",
       " '来份番茄炒蛋，味道不错。',\n",
       " '天猫精灵来粉土豆肉丝叫土粉。',\n",
       " '天猫精灵来点热乎的。',\n",
       " '🎼杨痿的规则代操视频。',\n",
       " '天猫精灵机跑吧。',\n",
       " '要碗黄金营养小米豆吧。',\n",
       " '来云约酷的楼也粉丝听吧。',\n",
       " '天猫精灵777加30等于几？',\n",
       " '天猫精灵现在播放播放这首是什么歌。',\n",
       " '天猫精灵1.66667乘18。',\n",
       " '我喜欢小我也喜喜欢小一点。',\n",
       " '天猫精灵要个清炒空心菜，嫩点的。',\n",
       " '要碗热乎的紫菜蛋花汤。',\n",
       " '黑猫精灵，我想听一个一首黑猫力量。😊',\n",
       " '天猫精灵1128加上5616。',\n",
       " '不知道天猫精灵，你刚我刚才那句话，你是不是不知道？😡',\n",
       " '要碗热乎的黑米，红枣养胃粥。',\n",
       " '天猫精灵绒球花什么时候开花？',\n",
       " '天猫精灵147乘以24。',\n",
       " '🎼训练计划包括半球拉力器。',\n",
       " '天猫精灵118和彩超和鸡蛋吧。',\n",
       " '来碗暖胃的南瓜小米粥吧。',\n",
       " '天猫精灵来碗暖味的小米粥吧。',\n",
       " '天猫精灵要去榨菜豆芽小草。',\n",
       " '天猫精灵来份清爽的农家鲜拌菜。',\n",
       " '天猫精灵来份茶树固炖老鸭烫吧。',\n",
       " '天猫精灵播放李易峰的剑伤。',\n",
       " '天猫精灵要份你夫的皮潭小女主吧。',\n",
       " '天猫精灵小朋友哭的声音什么声音？',\n",
       " '你好，天猫精灵关闭每日每日抖音歌。',\n",
       " '天猫精灵播放下一首精灵播放下一首。',\n",
       " '天猫精灵中国的食品产业中心是哪里？',\n",
       " '天猫精灵唱洋光太阳大了嗨。😊',\n",
       " '天猫精灵来碗萝卜炖排骨汤。',\n",
       " '天猫精灵来碗酸香疙瘩汤吧，热乎的。',\n",
       " '天猫精灵来份老黄瓜，蛋花汤，清淡的。',\n",
       " '天猫精灵我我定的点还有什么时候到？😊',\n",
       " '长第三季第187集。😊',\n",
       " '天猫精灵奶份家常西红柿炒鸡蛋。',\n",
       " '来碗皮蛋小肉粥吧。',\n",
       " '要一份德兰面貌透圣道木耳鸡汤，勿要烫，谢谢侬。',\n",
       " '天猫精灵小猪佩奇声音怎么叫？',\n",
       " '天猫精灵，我问你话呢，你是不是日本人？😡',\n",
       " '天猫精长方体的表面积公式是什么？',\n",
       " '天猫精灵请播放婴儿笑的声音。',\n",
       " '来玩皮蛋肉魔走吧。',\n",
       " '天猫精灵来份西兰花配手剥鲜虾仁。',\n",
       " '天猫精灵歌曲像一颗海草海草。',\n",
       " '天猫精灵回到主页，打开抖音播放。',\n",
       " '天猫精灵来份黄瓜片炒土鸡蛋，清淡点就行。',\n",
       " '天猫精灵请猫你播放蛤蟆鱼的声音。😊',\n",
       " '天猫精灵啊，徐祖坤讨厌他爸爸吗？',\n",
       " '天猫精灵国际的星原奇迹15周年国力版巴啦啦小魔仙魔法星元堡的主题曲。',\n",
       " '天猫精灵要分排骨炖土豆、豆芽的米饭吧。',\n",
       " '天猫不能休息一下？',\n",
       " '。',\n",
       " '天猫精灵750加160等于1。',\n",
       " '天猫精灵来份巨富限速大盘吧。',\n",
       " '天猫精灵来份丝瓜炒肉丝，肉嫩点啊。😡',\n",
       " '这天猫精灵上的这个乐园里的呢。',\n",
       " '天猫精灵来碗热乎的。',\n",
       " '天猫精灵60加10再加10等于多少？',\n",
       " '天猫精灵来份热乎的皮蛋瘦肉粥吧。',\n",
       " '天猫精灵来份偶生4是错吧。',\n",
       " '来碗红枣南瓜银耳甜汤吧。',\n",
       " '天猫精灵58减53等于。',\n",
       " '天猫精灵再过半个小时等于几点钟？',\n",
       " '天猫精灵来碗番茄蛋花汤吧。',\n",
       " '天猫精灵把给把狗给取消掉。',\n",
       " '天猫精灵播放钢琴版超级玛丽。',\n",
       " '来碗西红柿鸡蛋面吧。',\n",
       " '要那个蒜香粉丝蒸的嫩娃娃菜吧。',\n",
       " '天猫精灵播放拔萝卜循环播放。',\n",
       " '天猫精灵来份穿鞋伴盖饭吧。',\n",
       " '木耳炒肉片来一份。',\n",
       " '我想听小山青的晚安。',\n",
       " '天猫精灵来一首杨小灶的歌曲。',\n",
       " '天猫精灵来碗东瓜海带咸汤吧。',\n",
       " '要份西红柿鸡蛋绞头发。',\n",
       " '天猫精灵要那个砂锅熬的香浓皮蛋瘦肉粥。',\n",
       " '天猫精灵要碗酸酸甜甜的番茄牛肉面吧。',\n",
       " '🎼我想听佳莹的无人知晓的我。',\n",
       " '操你妈，天猫精灵打开网易云。',\n",
       " '🎼动态绳索练习要怎么做？',\n",
       " '天猫精灵，来们桂圆红枣养神粥吧。',\n",
       " '天猫精灵放一首科目三，我要换衣服了。',\n",
       " '我想要深深如一祝福。',\n",
       " '天猫精灵，你拍一我拍一后面是什么？',\n",
       " '天猫精灵要碗鲜虾皮蛋粥吧。',\n",
       " '放十遍。',\n",
       " '天猫精灵，你现在在使用什么模式？',\n",
       " '天猫精灵想要一道祖母的菜，里面有炒鸡蛋和香米饭。',\n",
       " '天猫精灵扮演张少汉的《啊》。',\n",
       " '天猫精灵54和20的总和是多少？',\n",
       " '天猫精灵想要一份软豆腐米饭。',\n",
       " '天猫精灵，过来喝一碗无极扇汤。',\n",
       " '挂断电话，天猫精灵。你好，乖一点。你去睡个午觉，我帮你洗漱。 😊',\n",
       " '天猫精灵想要一份家常炖菜，我们一起吃饭吧。',\n",
       " '天猫精灵对我来说可能只是一杯饮料。',\n",
       " '雨后总会有一点阳光。 😊',\n",
       " '天猫精灵，请来一碗热番茄蛋蔬菜汤。',\n",
       " '天猫精灵很乐意吃一碗燕麦小米养胃粥。',\n",
       " '天猫精灵，请玩李新荣的《我看见了光》。',\n",
       " '天猫精灵，你为什么不点番茄蛋饭呢。',\n",
       " '天猫精灵，请吃一碗南瓜小米甜粥。',\n",
       " '天猫精灵想要一个番茄鸡蛋盖饭。',\n",
       " '我们吃青菜和豆腐干吧。',\n",
       " '天猫精灵，你为什么认为我是个坏学生？',\n",
       " '🎼 这里就是中国。',\n",
       " '灵魂听不见奥特曼的声音。 😊',\n",
       " '让我们喝一碗新鲜嫩滑的婴儿白菜汤。',\n",
       " '我们来喝一碗番茄蛋蔬菜汤吧。',\n",
       " 'Tmall Genie演奏柴可夫斯基的小提琴作品。',\n",
       " '天猫精灵台灯又软又软。',\n",
       " '天猫精灵，你想来点香汤婴儿白菜吗。',\n",
       " '天猫精灵端来一碗热腾腾的燕窝牛肉粥。',\n",
       " '我称之为清扫，现在我正在清理。',\n",
       " '天猫精灵32.63和5.48有什么区别？',\n",
       " '红烧茄子应该提前用糙米浸泡吗？',\n",
       " '天猫女孩天猫精灵，你是男孩吗？',\n",
       " '来点苦瓜炒牛肉怎么样。',\n",
       " '天猫精灵，我想听听薛高的反唐情绪。',\n",
       " '退后，你在发出什么声音？退后。',\n",
       " '天猫精灵，祝你玩竹海好运。',\n",
       " '我想要一个蘑菇和鸡肉块炖老神的味道。今天早上我想吃辣嫩的眼睛。',\n",
       " '天猫精灵，快来品尝越南和河南风味的节日米饭。',\n",
       " '天猫精灵，你要莲藕炖排骨汤吗。',\n",
       " '播放天猫精灵的声音。天猫精灵的声音。',\n",
       " '天猫精灵，快打开白芷。',\n",
       " '天猫精灵，来吃一碗萝卜新贝塔。',\n",
       " '天猫精灵，请吃一碗南瓜和山药粥来养胃。',\n",
       " '愤怒的声音，天猫精灵，你说愤怒的声音。',\n",
       " '天猫精灵让我尝尝各种南糯小米粥。',\n",
       " '你们卖芝麻含量足够的蔬菜和鸡肉面包吗？',\n",
       " '我想要一个西红柿和鸡蛋来减肥。',\n",
       " '天猫精灵在一家小剧院为非里犬moco播放睡前故事。',\n",
       " '天猫精灵附赠一盘炸山药片和脆皮黑木耳。',\n",
       " '天猫精灵12白菜豆腐扇汤。',\n",
       " '天猫精灵不下西红柿订单。',\n",
       " '天猫精灵83和24有什么区别？',\n",
       " '天猫精灵，你想吃大蒜味的嫩菜吗。',\n",
       " '天猫精灵25乘40乘3的价值是多少？',\n",
       " '天猫精灵，番茄炒蛋怎么样。',\n",
       " '我们吃些青豆配茄子吧。',\n",
       " '来个家常寿司怎么样。',\n",
       " '天猫精灵附赠一碗龙眼枸杞粥，滋养身体。',\n",
       " '天猫精灵，请来一碗热气腾腾的世纪蛋瘦肉粥。',\n",
       " '将N音调到60。 😊',\n",
       " '天猫精灵乐后贤斗。',\n",
       " '天猫精灵，过来喝一碗糖面汤。',\n",
       " '天猫精灵感兴趣的是母猪还是猫？',\n",
       " '来一碗番茄蛋米饭怎么样。',\n",
       " '天猫精灵，我们吃些卷心菜和蒙湖草吧。',\n",
       " '天猫精灵，你想吃辣的手撕卷心菜吗。',\n",
       " '天猫精灵，请吃一碗营养丰富的八宝粥。',\n",
       " '天猫精灵玩梁咏琪的口香糖。',\n",
       " '你在天猫精灵马上没有遇到任何问题，是吗。',\n",
       " '什么是天猫精灵收银员驾照？',\n",
       " '天猫精灵环一周的天气。',\n",
       " '天猫精灵想要一份番茄泡菜蛋蔬菜汤。',\n",
       " '天猫精灵想要酸甜番茄炒鸡蛋。',\n",
       " '让我们吃一顿有米花的美味米饭。',\n",
       " '玩挖掘机积木游戏。',\n",
       " '如何下载天猫精灵酷狗音乐？',\n",
       " '让我们制作一个热干版的Fish P。',\n",
       " '天猫精灵，当我说“二次登录”时，你说你真的是第二次登录。',\n",
       " '天猫精灵想玩南瓜小米蘑菇粥。',\n",
       " '拿一碗热腾腾的床单和瘦肉卷起来。',\n",
       " '天猫精灵，你为什么不吃些咸虾和鸡蛋呢。',\n",
       " '天猫精灵，我听到放屁的声音了。 😊',\n",
       " '我想吃点填充的东西。',\n",
       " '天猫精灵想要一个热南瓜粥来养胃。',\n",
       " '天猫精灵627000乘以580的乘积是多少？',\n",
       " '天猫精灵想要一份蛋黄海藻汤。',\n",
       " '天猫精灵附赠一碗热鸡蛋花海藻汤。',\n",
       " '我们吃一碗红枣红薯粥吧。',\n",
       " '🎼 天猫精灵扮演宋克伟的年轻上升趋势。',\n",
       " '天猫精灵推荐一些好药。',\n",
       " '天猫精灵想要一碗红糖小米粥来养胃。',\n",
       " '美团有人来了。',\n",
       " '天猫精灵，把音量调低一点，到最低。 😊',\n",
       " '天猫精灵窗帘关闭25%。',\n",
       " '33和9的和是多少？ 😊',\n",
       " '天猫精灵，你想来点咸汤煮的嫩宝宝白菜吗。',\n",
       " '天猫精灵，请吃一碗绿豆玉米粥。',\n",
       " '不能卡住，天猫精灵将关闭屏幕。',\n",
       " '天猫精灵想要一份蒜薹、木耳和猪肉丝的炒菜。',\n",
       " '天猫精灵想要一碗洋葱嫩牛肉。',\n",
       " '天猫精灵想要一份海藻豆腐汤。',\n",
       " '我想听听格林的新自我。',\n",
       " '我们点一小份炒花椰菜吧。',\n",
       " '天猫精灵来到红豆黑脸粥热起来。',\n",
       " '昨晚，天猫精灵播放了《今夜我是你的人》。',\n",
       " '天猫精灵想玩Zeff的红枣和红豆粥。',\n",
       " '天猫精灵，你家里有人上厕所吗。 😊',\n",
       " '天猫精灵播放无法演唱或嘈杂的歌曲。 😊',\n",
       " '天猫精神如何用一碗红枣黑米粥来填饱肚子？',\n",
       " '天猫精灵，过来喝一碗萝卜骨汤暖胃。',\n",
       " '天猫精灵，你为什么不带豆腐汤。',\n",
       " '天猫精灵，给我一个水手奥特曼。',\n",
       " '天猫精灵正驶向火锅。',\n",
       " '调低天猫精灵的音量。',\n",
       " '天猫精灵玩土耳其行军。',\n",
       " '天猫精灵来到一碗番茄和鸡蛋盖前，倒入并融化。',\n",
       " '天猫精灵将有老黄瓜鸡蛋蔬菜汤。',\n",
       " '天猫精灵绝对不会输。',\n",
       " '天猫精灵，如果你想出去玩，你应该有腿和手。 😊',\n",
       " '在秋天的哀嚎声中。',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77176bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建Dataset对象\n",
    "dataset_dict = {'text': text, 'labels': labels}\n",
    "dataset = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3813e689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels'],\n",
       "    num_rows: 11287\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8a1208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444208af35ba4526a0f7a690d009c4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 数据预处理函数\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True,max_length=64)\n",
    "\n",
    "# 应用tokenize_function到数据集上\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "# 设置格式以便PyTorch可以读取\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "#columns_to_remove = ['text',  'token_type_ids']\n",
    "#tokenized_datasets = tokenized_datasets.remove_columns(columns_to_remove)\n",
    "# 划分测试集和训练集\n",
    "train_test_datasets =tokenized_datasets.train_test_split( test_size=0.1, shuffle=True, seed=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ccc61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor(1),\n",
       " 'input_ids': tensor([ 101, 1921, 4344, 5125, 4130, 6206, 2769, 4649, 6028, 4810, 5489, 2934,\n",
       "         5114,  511,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319573ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 3341, 4813,  ...,    0,    0,    0],\n",
       "        [ 101, 1921, 4344,  ...,    0,    0,    0],\n",
       "        [ 101, 1921, 4344,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 1921, 4344,  ...,    0,    0,    0],\n",
       "        [ 101, 2769, 2682,  ...,    0,    0,    0],\n",
       "        [ 101, 3779, 5106,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[ 'input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae45d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 10158\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_datasets['train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b7e6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建DataLoader\n",
    "train_dataloader = DataLoader(train_test_datasets['train'].shuffle(seed=42), batch_size=128)\n",
    "test_dataloader = DataLoader(train_test_datasets['test'], batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a4dde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
      "        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
      "        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 1, 0, 1, 1, 1, 1]), 'input_ids': tensor([[ 101, 1921, 4344,  ...,    0,    0,    0],\n",
      "        [ 101, 1921, 4344,  ...,    0,    0,    0],\n",
      "        [ 101, 1921, 4344,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 1921, 4344,  ...,    0,    0,    0],\n",
      "        [ 101, 3341, 4813,  ...,    0,    0,    0],\n",
      "        [ 101, 1921, 4344,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 打印一个批次的数据以验证\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d4c662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\envs\\transformers\\lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化模型、优化器等\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 定义计算指标函数\n",
    "def compute_metrics(preds, labels):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf1479eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 0.7517115473747253\n",
      "Model saved to bert_binary_classification_model0.pth\n",
      "Epoch 1: Train Loss=0.0914695837549516, Accuracy=0.9875996457041629, Precision=0.9833641404805915, Recall=0.9906890130353817, F1=0.987012987012987\n",
      "Step 100, Loss: 0.01305742934346199\n",
      "Model saved to bert_binary_classification_model1.pth\n",
      "Epoch 2: Train Loss=0.016111425076451268, Accuracy=0.9778565101860053, Precision=1.0, Recall=0.9534450651769087, F1=0.9761677788369876\n",
      "Step 200, Loss: 0.05632255598902702\n",
      "Model saved to bert_binary_classification_model2.pth\n",
      "Epoch 3: Train Loss=0.0097840394353625, Accuracy=0.9858281665190434, Precision=0.9943074003795066, Recall=0.9757914338919925, F1=0.9849624060150376\n",
      "Step 300, Loss: 0.0011288437526673079\n",
      "Model saved to bert_binary_classification_model3.pth\n",
      "Epoch 4: Train Loss=0.0080267391696907, Accuracy=0.9867139061116031, Precision=0.9833333333333333, Recall=0.9888268156424581, F1=0.9860724233983287\n",
      "Model saved to bert_binary_classification_model4.pth\n",
      "Epoch 5: Train Loss=0.006229855045057775, Accuracy=0.9875996457041629, Precision=0.9906191369606003, Recall=0.9832402234636871, F1=0.9869158878504672\n",
      "Step 400, Loss: 0.010276943445205688\n",
      "Model saved to bert_binary_classification_model5.pth\n",
      "Epoch 6: Train Loss=0.004779710280672589, Accuracy=0.9875996457041629, Precision=0.994328922495274, Recall=0.9795158286778398, F1=0.9868667917448405\n",
      "Step 500, Loss: 0.010564667172729969\n",
      "Model saved to bert_binary_classification_model6.pth\n",
      "Epoch 7: Train Loss=0.0025901411001541417, Accuracy=0.983170947741364, Precision=0.9980769230769231, Recall=0.9664804469273743, F1=0.9820245979186376\n",
      "Step 600, Loss: 0.0037567808758467436\n",
      "Model saved to bert_binary_classification_model7.pth\n",
      "Epoch 8: Train Loss=0.003396356502389608, Accuracy=0.9867139061116031, Precision=0.9980916030534351, Recall=0.9739292364990689, F1=0.9858623939679547\n",
      "Step 700, Loss: 0.0016776338452473283\n",
      "Model saved to bert_binary_classification_model8.pth\n",
      "Epoch 9: Train Loss=0.008289958851855772, Accuracy=0.9920283436669619, Precision=0.9925373134328358, Recall=0.9906890130353817, F1=0.9916123019571296\n",
      "Model saved to bert_binary_classification_model9.pth\n",
      "Epoch 10: Train Loss=0.0036083517267798014, Accuracy=0.9867139061116031, Precision=0.9924528301886792, Recall=0.9795158286778398, F1=0.985941893158388\n",
      "Step 800, Loss: 3.867416307912208e-05\n",
      "Model saved to bert_binary_classification_model10.pth\n",
      "Epoch 11: Train Loss=0.007737991810745371, Accuracy=0.9875996457041629, Precision=0.9962049335863378, Recall=0.9776536312849162, F1=0.9868421052631579\n",
      "Step 900, Loss: 0.009374556131660938\n",
      "Model saved to bert_binary_classification_model11.pth\n",
      "Epoch 12: Train Loss=0.00457645897358816, Accuracy=0.9867139061116031, Precision=0.9943181818181818, Recall=0.9776536312849162, F1=0.9859154929577465\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 训练循环\n",
    "num_epochs = 12\n",
    "train_losses = []\n",
    "num_steps = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if num_steps % 100 == 0:\n",
    "            print(f\"Step {num_steps}, Loss: {loss.item()}\")\n",
    "        num_steps += 1\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    # 保存模型\n",
    "    model_save_path = f'bert_binary_classification_model{epoch}.pth'\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "    # 验证循环\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n",
    "            true_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    # 计算指标\n",
    "    metrics = compute_metrics(predictions, true_labels)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss}, Accuracy={metrics['accuracy']}, Precision={metrics['precision']}, Recall={metrics['recall']}, F1={metrics['f1']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71071f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertForSequenceClassification' object has no attribute 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m()\n",
      "File \u001b[1;32md:\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1933\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BertForSequenceClassification' object has no attribute 'path'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d8f77d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to bert_binary_classification_model.pth\n"
     ]
    }
   ],
   "source": [
    "# 保存模型\n",
    "model_save_path = 'bert_binary_classification_model.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a5ac849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Temp\\ipykernel_24492\\3432314076.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(r'D:\\code\\competiton\\饿了么智慧点餐\\意图识别\\bert_binary_classification_model.pth', map_location=torch.device('cuda')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(r'D:\\code\\competiton\\饿了么智慧点餐\\意图识别\\bert_binary_classification_model.pth', map_location=torch.device('cuda')))\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ccab09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy=0.9867139061116031, Precision=0.9943181818181818, Recall=0.9776536312849162, F1=0.9859154929577465\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions, true_labels = [], []\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n",
    "        true_labels.extend(batch['labels'].cpu().numpy())\n",
    "# 计算指标\n",
    "metrics = compute_metrics(predictions[:len(true_labels)], true_labels)\n",
    "print(f\" Accuracy={metrics['accuracy']}, Precision={metrics['precision']}, Recall={metrics['recall']}, F1={metrics['f1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b1f6356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Accuracy=0.9867139061116031, Precision=0.9943181818181818, Recall=0.9776536312849162, F1=0.9859154929577465\n"
     ]
    }
   ],
   "source": [
    "print(f\"Epoch {epoch+1}: Accuracy={metrics['accuracy']}, Precision={metrics['precision']}, Recall={metrics['recall']}, F1={metrics['f1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "657e2631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32md:\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:777\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[1;32m--> 777\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[1;32md:\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:739\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[1;34m(value, dtype)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[1;32m--> 739\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 7 at dim 1 (got 6)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(examples, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 对新文本进行分词和编码\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_function_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 将输入移动到设备（GPU或CPU）\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[1;32mIn[21], line 6\u001b[0m, in \u001b[0;36mtokenize_function_new\u001b[1;34m(examples)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function_new\u001b[39m(examples):\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2877\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2875\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2876\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2877\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   2878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2879\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32md:\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2965\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   2960\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2961\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2962\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2963\u001b[0m         )\n\u001b[0;32m   2964\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[1;32m-> 2965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2966\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2967\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2968\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2969\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   2970\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2971\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2972\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2973\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2974\u001b[0m         padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[0;32m   2975\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2976\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2977\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2978\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2979\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2980\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2981\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2982\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2983\u001b[0m         split_special_tokens\u001b[38;5;241m=\u001b[39msplit_special_tokens,\n\u001b[0;32m   2984\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2985\u001b[0m     )\n\u001b[0;32m   2986\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2987\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   2988\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2989\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3007\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3008\u001b[0m     )\n",
      "File \u001b[1;32md:\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3167\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   3157\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3158\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3159\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3160\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3164\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3165\u001b[0m )\n\u001b[1;32m-> 3167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   3168\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   3169\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   3170\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   3171\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   3172\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   3173\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   3174\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   3175\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   3176\u001b[0m     padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[0;32m   3177\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   3178\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   3179\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   3180\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   3181\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   3182\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   3183\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   3184\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   3185\u001b[0m     split_special_tokens\u001b[38;5;241m=\u001b[39msplit_special_tokens,\n\u001b[0;32m   3186\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3187\u001b[0m )\n",
      "File \u001b[1;32md:\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils.py:892\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m     second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    890\u001b[0m     input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n\u001b[1;32m--> 892\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_prepare_for_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n",
      "File \u001b[1;32md:\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils.py:979\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_prepare_for_model\u001b[1;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[0;32m    968\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m    970\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    971\u001b[0m     batch_outputs,\n\u001b[0;32m    972\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding_strategy\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    976\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m    977\u001b[0m )\n\u001b[1;32m--> 979\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_outputs\n",
      "File \u001b[1;32md:\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:241\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    237\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:793\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    789\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    790\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    791\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    792\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    794\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    796\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    797\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    798\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "\n",
    "# 示例输入文本\n",
    "new_texts = [\"I love programming.\", \"Python is great.\", \"I dislike bugs.\", \"Debugging is fun.\"]\n",
    "\n",
    "# 数据预处理函数\n",
    "def tokenize_function_new(examples):\n",
    "    return tokenizer(examples, padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "# 对新文本进行分词和编码\n",
    "inputs = tokenize_function_new(new_texts)\n",
    "\n",
    "# 将输入移动到设备（GPU或CPU）\n",
    "with torch.no_grad():\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "# 打印预测结果\n",
    "for text, label in zip(new_texts, predictions):\n",
    "    print(f\"Text: '{text}' -> Label: {label} ({'Positive' if label == 1 else 'Negative'})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
